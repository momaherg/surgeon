# NPT Training Configuration - Small Model for Testing
model:
  base_model_name: "gpt2"  # Much smaller model for testing
  npt_layers: "upper_half"  # Options: "all", "upper_half", "lower_half", or list of layer indices
  rank: 8  # Reduced rank for memory efficiency
  modulation_scale: 0.1  # Scaling factor for weight deltas
  
training:
  batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 0.0001  # 1e-4
  num_epochs: 3
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Loss weights
  equivalence_weight: 1.0
  regularization_weight: 0.01  # For low-magnitude Î”W constraint
  
  # Logging
  logging_steps: 10
  prediction_logging_steps: 50
  eval_steps: 100
  save_steps: 200
  
data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-raw-v1"  # Smaller dataset
  max_length: 256  # Reduced sequence length
  num_train_samples: 1000  # For initial experiments
  num_eval_samples: 100
  
wandb:
  project: "npt-equivalence-pretraining"
  entity: null  # Set your WandB entity
  tags: ["npt", "phase1", "equivalence", "small_model"]
  
paths:
  output_dir: "./outputs"
  cache_dir: "./cache"
  checkpoint_dir: "./checkpoints"
